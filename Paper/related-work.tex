% • What is the challenge of your study problem? 
% • What is the existing method for your study problem (papers, online search results, less than 5)
%  • What is their strength and weakness
%  • How is your work similar or different from them?

\section{Related Work}
\label{sec:related-work}

The core question of this work aims to evaluate
the ability of LLMs to produce correct Coq proofs
despite input obfuscation.

\subsection
{Evaluating LLM-generated code in Coq}
Proof Automation with Large Language Models

\subsection
{Creating datasets}
Learning to Prove Theorems via
Interacting with Proof Assistants


\subsection
{Methodology in Coq against other languages}

Verifying the correctness of Coq proofs
compared against LLM-generated code for other
languages is a relatively closed problem
given the well-defined proof goal and the
strictness of the Coq theorem prover.
CodeJudge (Tong and Zhang, 2024) explores
alternative approaches of verification
employed to 

CodeJudge: Evaluating Code Generation 
with Large Language Models