% • What is the challenge of your study problem? 
% • What is the existing method for your study problem (papers, online search results, less than 5)
%  • What is their strength and weakness
%  • How is your work similar or different from them?

\section{Related Work}
\label{sec:related-work}


\paragraph{Creating datasets}
To properly evaluate LLM performance on
Coq code generation requires a corpus of well-structured
proofs of theorems. For some language processing tasks
there are large bodies of existing works that can be leveraged for this purpose, 
with proper consideration for authorized and ethical reuse.
News media can serve as authoritative examples of
well-formatted natural language, and for popular programming
languages such as Python, C, and JavaScript many large
open-source projects exist.
As theorem provers are relevant to a much smaller audience, the pool of quality examples in Coq is
comparatively small.
\citet{learningtoprove} compose a bespoke solution
to this problem. They collected over 70,000 human-written proofs
from over 100 different Coq projects to produce CoqGym,
a dataset and training environment for deep learning models.
This work addresses the issue of scale required for LLM training,
but any LLMs produced
after the publication of this work
may have been partially or totally exposed to this set,
procluding it from use to evaluate performance.
Our dataset intends to address this challenge
through mutating variables included
in existing datasets.

\paragraph{Evaluating LLM-generated code in Coq}

\citep{proofautomationwithllms} introduces PALM,
a novel approach for generating Coq proofs with LLMs.
PALM can utilize an existing LLM, such as GPT-4 or Llama-3,
as a baseline for proof synthesis and supplies repair tools
to refine a model's answer.
Contrasted with existing approaches that are able
to solve 15\%-20\% of theorems presented to them,
PALM achieves a proof success rate of 32\%-43\% 
depending on the base model used.
Without PALM, none of the base models can
complete more than 7\% of the test set.
PALM uses the previously mentioned CoqGym test set
for performance evaluation, 
a set that was published 5 years prior. 
In their paper, \citeauthor{proofautomationwithllms} describe the steps 
they took to avoid testing with data 
that the underlying LLMs had already seen.  
While the authors removed biased test data they were aware of, 
they used publicly available models as a starting point, 
meaning there is no guarantee that the baseline models 
they used have not been tainted with undisclosed training data. 
One of the base LLM models used in evaluation is Llama-3, 
which has allegations of unethical data collection 
levied against its creators \cite{llama-case}. 
The probability of a model being tainted 
with the training data in the CoqGym set increases with time, 
meaning that either new datasets for Coq code will be required, 
or a means of mutating existing datasets will be required; 
the latter being the solution we pursue.


%Learning to Prove Theorems via
%Interacting with Proof Assistants

\paragraph{Methodology in Coq against other languages}
In our target domain, Coq proofs, 
we are interested in the formal correctness of our LLM output.
For Proof scripts in a theorem prover,
the correctness of the generated code is proved true
simply by running the proof in the theorem prover.
If code type-checks without an error it is guaranteed to be correct.
In typical LLM code generation,
there are a few criteria for judging the quality of LLM generated code.  
A common metric for program correctness is behavioral correctness,
which can be accomplished by comparing the outputs of generated code
and a known correct program on the problem specification such as through unit testing.
Text similarity between ground truth code and the LLM output can be used
in cases where an LLM may produce code that is conceptually close
to correct code but features syntax errors that may render an
otherwise accurate solution unable to be compiled.
\citet{CodeJudge} present CodeJudge, a novel technique
used to evaluate LLM-generated code using another LLM.
The judging LLM employs "slow thinking" to arrive at a
thorough evaluation of the generated code,
which unlike unit tests or code similarity can consider
more nuanced notions of correctness and can have its
criteria adjusted.
We chose to use proof correctness as evaluation criteria
based upon the objectives of this research.
If the use case for LLM proof generation is to
collaborate with a human programmer,
more flexible evaluation criteria, as those above, may be more relevant. 

