% • What is the challenge of your study problem? 
% • What is the existing method for your study problem (papers, online search results, less than 5)
%  • What is their strength and weakness
%  • How is your work similar or different from them?

\section{Related Work}
\label{sec:related-work}


\paragraph{Creating datasets}
Popular programming
languages such as Python, have large sets of publicly available code making it relatively simple to construct training and testing datasets.
As theorem provers are relevant to a much smaller audience, the pool of quality examples in Coq is
comparatively small.
\citet{learningtoprove} compose CoqGym,
a bespoke dataset and training environment for deep learning models
composed of 70,000 human-written proofs from over 100 projects.
This work addresses the issue of scale required for LLM training,
but any LLMs produced
after the publication of this work
may have been exposed to it and cannot be judged blindly.
\textbf{Our dataset intends to address this challenge through mutating variables included in existing datasets.}

\paragraph{Evaluating LLM-generated code in Coq}
\citet{proofautomationwithllms} introduces PALM,
a novel approach for generating Coq proofs with LLMs.
PALM can utilize an existing LLM, such as GPT-4 or Llama-3,
as a baseline for proof synthesis and supplies repair tools
to refine a model's answer.
Contrasted with existing approaches that are able
to solve 15\%-20\% of theorems presented to them,
PALM achieves a proof success rate of 32\%-43\% 
depending on the base model used.
Without PALM, none of the base models can
complete more than 7\% of the test set.
PALM uses the previously mentioned CoqGym test set
for performance evaluation, 
a set that was published 5 years prior. 
In their paper, \citeauthor{proofautomationwithllms} describe the steps 
they took to avoid testing with data that the underlying LLMs had already seen.
This task becomes more challanging to do over time, though, as the probability that 
a publicly available model is exposed to the CoqGym test set increases with time. 
Giving rise to the need for new or mutated datasets. 


%Learning to Prove Theorems via
%Interacting with Proof Assistants

\paragraph{Methodology in Coq against other languages}
In our target domain, Coq proofs,
code can be shown to be correct by simply type-checking in the theorem prover.
LLM code generation for other languages
has been judged by several other criteria for correctness,
including behavioral correctness and code similarity.
\citet{CodeJudge} present CodeJudge, a novel technique
used to evaluate LLM-generated code using another LLM.
The judging LLM employs "slow thinking" to arrive at a
thorough evaluation of the generated code,
which unlike unit tests or code similarity can consider
more nuanced notions of correctness and can have its
criteria adjusted.
We chose to use proof correctness as evaluation criteria
based upon the objectives of this research.
If the use case for LLM proof generation is to
collaborate with a human programmer,
more flexible evaluation criteria, as those above, may be more relevant. 

