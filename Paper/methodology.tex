%  • Your learning algorithms
%  • Describe the model, its mechanism, how it works in your scenario
%  • Learning objective in formal language and math equations (refers to textbook and lecture notes)
%  • What is the strength and weakness of your model?

\section{Methodology}
\label{sec:methodology}

To isolate the role of human-readable identifiers (such as \texttt{Theorem}, \texttt{Lemma}, \texttt{Inductive}, etc.), we applied a preprocessing pipeline:

\begin{itemize}
  \item \textbf{Identifier Detection and Renaming:} We located all top-level identifiers associated with theorems, lemmas, facts, and inductive constructors, then renamed them to opaque, non-descriptive tokens. During this process, we also had to repair certain automatically generated names created by Coq, such as those used for inductive hypotheses. In addition, we preserved a small set of exceptional identifiers from the \textit{Logical Foundations} source, (\texttt{com}, \texttt{com\_aux}, \texttt{f\_equal}, \texttt{string}, and \texttt{app\_nil\_r}), which caused correctness or dependency issues when renamed.
  \item \textbf{Comment Removal:} All comments were stripped from the files to prevent LLMs from accessing potentially informative natural language.
  \item \textbf{Module Filtering:} Entire modules were excluded from the prompt context to avoid issues with internal naming scopes and to simplify parsing.
\end{itemize}

By comparing LLM performance before and after identifier renaming, we evaluated whether the models exploit these names as semantic cues.

\subsection{Evaluation Objective}

Our goal was not to train the models, but to evaluate their robustness under controlled perturbations in source code. Although we did not formulate a formal learning objective with a loss function, our evaluation criterion can be thought of as a binary classification over model outputs:

\[
\text{Success}(x) =
\begin{cases}
1, & \text{if Coq compiler accepts } x \\
0, & \text{otherwise}
\end{cases}
\]

\noindent where \( x \) is the full Coq file with the model-generated proof inserted. We compared the acceptance rates across versions with original vs. renamed identifiers.

Letting \( D = \{x_i\} \) be a dataset of proof goals and \( f_\theta(x_i) \) the LLM's generated proof for input \( x_i \), the empirical success rate is:

\[
\text{Accuracy} = \frac{1}{|D|} \sum_{i=1}^{|D|} \text{Success}(f_\theta(x_i))
\]

\subsection{Strengths and Weaknesses}
We consider this pipeline broadly successful: it allowed us to systematically remove semantic hints embedded in names while maintaining proof validity across the \textit{Logical Foundations} files.

A key strength of our approach is its generality: it can be applied to a wide range of Coq source files with minimal manual intervention. This made it feasible to process an entire textbook-scale dataset.

However, we encountered several limitations that required pragmatic compromises.
First, Coq's support for nested modules led to name shadowing and scoping ambiguities during transformation.
To address this, we excluded all code defined within Module blocks, which limited our dataset coverage.
Second, as described previously, a small number of identifiers were excluded from renaming due to internal dependencies in the \textit{Logical Foundations} textbook. Some of these were recreated for pedagogical purposes or caused scoping issues and likely will not pose the same challenge in other real-world datasets.

Looking forward, we believe that more principled approaches to identifier transformation are possible. One promising direction is leveraging Coq's extraction mechanism to perform semantically sound renaming while avoiding scoping issues.
We also see value in extending our transformation strategy beyond naming.
Specifically, we explored (but were unable to finish) applying logical equivalences, which could help evaluate whether LLMs can recognize and generalize over structurally identical but syntactically different proof goals.
